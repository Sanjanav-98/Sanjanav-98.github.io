Visual Assistancefor the blind and Visually Impaired 

This paper aims at assisting visually impairedpeople through Deep Learning (DL) by providing a system thatcan describe the surroundings as well as answer questions aboutthe surroundings of the user. The system majorly consists of twomodels, an Image Captioning (IC) model, and a Visual QuestionAnswering (VQA) model. The IC model is a Convolutional Neu-ral Network and Recurrent Neural Network based architecturethat incorporates a form of attention while captioning. Thispaper proposes two models, Multi-Layer Perceptron based andLong Short Term Memory (LSTM) based, for the VQA task thatanswer questions related to the input image. The IC model hasachieved an average BLUE-1 score of 0.46. The LSTM basedVQA model has given an overall accuracy of 47%. These twomodels are integrated along with Speech to Text and Text toSpeech components to form a single system that works in real-time.

[Link to the IEEE paper](https://ieeexplore.ieee.org/document/9342481)

![alt text](/assets/img/Visual_Assistance_Paper-1.png)
![alt text](/assets/img/Visual_Assistance_Paper-2.png)
![alt text](/assets/img/Visual_Assistance_Paper-3.png)
![alt text](/assets/img/Visual_Assistance_Paper-4.png)
![alt text](/assets/img/Visual_Assistance_Paper-5.png)
![alt text](/assets/img/Visual_Assistance_Paper-6.png)
![alt text](/assets/img/Visual_Assistance_Paper-7.png)
