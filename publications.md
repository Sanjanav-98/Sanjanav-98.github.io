## PUBLICATIONS

### Paper-1 

Visual Assistance for the blind and Visually Impaired 

This paper aims at assisting visually impairedpeople through Deep Learning (DL) by providing a system thatcan describe the surroundings as well as answer questions aboutthe surroundings of the user. The system majorly consists of twomodels, an Image Captioning (IC) model, and a Visual QuestionAnswering (VQA) model. The IC model is a Convolutional Neu-ral Network and Recurrent Neural Network based architecturethat incorporates a form of attention while captioning. Thispaper proposes two models, Multi-Layer Perceptron based andLong Short Term Memory (LSTM) based, for the VQA task thatanswer questions related to the input image. The IC model hasachieved an average BLUE-1 score of 0.46. The LSTM basedVQA model has given an overall accuracy of 47%. These twomodels are integrated along with Speech to Text and Text toSpeech components to form a single system that works in real-time.

[Link to the IEEE paper](https://ieeexplore.ieee.org/document/9342481)

![alt text](/assets/img/Visual_Assistance_Paper-1.png)
![alt text](/assets/img/Visual_Assistance_Paper-2.png)
![alt text](/assets/img/Visual_Assistance_Paper-3.png)
![alt text](/assets/img/Visual_Assistance_Paper-4.png)
![alt text](/assets/img/Visual_Assistance_Paper-5.png)
![alt text](/assets/img/Visual_Assistance_Paper-6.png)
![alt text](/assets/img/Visual_Assistance_Paper-7.png)


### Paper-2

This paper proposes trajectory-based human action recognition using Center Symmetric-Local Binary Pattern (CS- LBP) (structural descriptor). Histogram of Optical Flow (HOF) and Motion Boundary Histogram (MBH), computed along the trajectory of the action, have been taken into account as motion descriptors. Additionally, the trajectory shape descriptor is ex- tracted. Harris corner is used to extract the key points. You Only Look Once (YOLO) has been employed to localize the human, for the KTH dataset, reducing the number of key points. Finally, the human actions are classified using K-means and Support Vector Machine (SVM) algorithms. The evaluation is performed on KTH and YouTube Action datasets, obtaining an accuracy of 91.6% and 90.4% respectively.

[Link to the IEEE paper](https://ieeexplore.ieee.org/document/9342248)

![alt text](/assets/img/Research_Project_Paper_Latest-1.png)
![alt text](/assets/img/Research_Project_Paper_Latest-2.png)
![alt text](/assets/img/Research_Project_Paper_Latest-3.png)
![alt text](/assets/img/Research_Project_Paper_Latest-4.png)
![alt text](/assets/img/Research_Project_Paper_Latest-5.png)
![alt text](/assets/img/Research_Project_Paper_Latest-6.png)
